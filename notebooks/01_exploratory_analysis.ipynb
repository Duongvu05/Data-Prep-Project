{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a061da0",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "## Discovering the Story Behind Our Data\n",
    "\n",
    "**Objective**: Perform initial exploration of raw data to understand its characteristics, identify quality issues, and set the foundation for our data storytelling narrative.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ab7700",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d72983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "\n",
    "# Custom modules\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "from utils.data_utils import load_data, get_data_quality_report\n",
    "from visualization.storytelling_viz import StorytellingVisualizer\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697d0cfd",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "\n",
    "Load your raw dataset here. Replace the file path with your actual data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588d697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "# TODO: Replace with your actual data file path\n",
    "# raw_data = load_data('../data/raw/your_dataset.csv')\n",
    "\n",
    "# For demonstration, create sample data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "raw_data = pd.DataFrame({\n",
    "    'feature_1': np.random.normal(100, 15, n_samples),\n",
    "    'feature_2': np.random.exponential(2, n_samples),\n",
    "    'feature_3': np.random.choice(['A', 'B', 'C', 'D'], n_samples),\n",
    "    'feature_4': np.random.uniform(0, 1, n_samples),\n",
    "    'target': np.random.choice([0, 1], n_samples)\n",
    "})\n",
    "\n",
    "# Introduce some data quality issues for demonstration\n",
    "# Missing values\n",
    "missing_indices = np.random.choice(n_samples, size=int(0.1 * n_samples), replace=False)\n",
    "raw_data.loc[missing_indices, 'feature_1'] = np.nan\n",
    "\n",
    "# Outliers\n",
    "outlier_indices = np.random.choice(n_samples, size=int(0.05 * n_samples), replace=False)\n",
    "raw_data.loc[outlier_indices, 'feature_2'] = raw_data['feature_2'].quantile(0.95) * 5\n",
    "\n",
    "# Duplicates\n",
    "raw_data = pd.concat([raw_data, raw_data.iloc[:50]], ignore_index=True)\n",
    "\n",
    "print(f\"📊 Data loaded successfully!\")\n",
    "print(f\"Shape: {raw_data.shape}\")\n",
    "print(f\"Memory usage: {raw_data.memory_usage(deep=True).sum() / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef79dee4",
   "metadata": {},
   "source": [
    "## 3. Initial Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d77413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the dataset\n",
    "print(\"=== DATASET INFO ===\")\n",
    "print(f\"Shape: {raw_data.shape}\")\n",
    "print(f\"Columns: {list(raw_data.columns)}\")\n",
    "print(\"\\n=== DATA TYPES ===\")\n",
    "print(raw_data.dtypes)\n",
    "print(\"\\n=== FIRST 5 ROWS ===\")\n",
    "display(raw_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daa1d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"=== STATISTICAL SUMMARY ===\")\n",
    "display(raw_data.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48de3719",
   "metadata": {},
   "source": [
    "## 4. Data Quality Assessment\n",
    "\n",
    "This is where we start building our story - identifying the problems with raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df51135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive data quality report\n",
    "quality_report = get_data_quality_report(raw_data)\n",
    "\n",
    "print(\"=== DATA QUALITY REPORT ===\")\n",
    "print(f\"📏 Shape: {quality_report['shape']}\")\n",
    "print(f\"🔄 Duplicates: {quality_report['duplicates']}\")\n",
    "print(f\"💾 Memory Usage: {quality_report['memory_usage'] / 1024:.2f} KB\")\n",
    "print(\"\\n❌ Missing Values:\")\n",
    "for col, missing in quality_report['missing_values'].items():\n",
    "    if missing > 0:\n",
    "        percentage = quality_report['missing_percentage'][col]\n",
    "        print(f\"  {col}: {missing} ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\n📊 Column Types:\")\n",
    "print(f\"  Numeric: {len(quality_report['numeric_columns'])}\")\n",
    "print(f\"  Categorical: {len(quality_report['categorical_columns'])}\")\n",
    "print(f\"  Datetime: {len(quality_report['datetime_columns'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dafe12d",
   "metadata": {},
   "source": [
    "## 5. Visual Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067ccce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations to highlight data quality issues\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Raw Data Quality Issues - The Story Begins', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Missing values heatmap\n",
    "sns.heatmap(raw_data.isnull(), cbar=True, ax=axes[0, 0], cmap='Reds')\n",
    "axes[0, 0].set_title('Missing Values Pattern')\n",
    "axes[0, 0].set_xlabel('Columns')\n",
    "axes[0, 0].set_ylabel('Rows (sample)')\n",
    "\n",
    "# 2. Distribution of numeric features\n",
    "numeric_cols = raw_data.select_dtypes(include=[np.number]).columns\n",
    "if len(numeric_cols) > 0:\n",
    "    raw_data[numeric_cols[0]].hist(bins=30, ax=axes[0, 1], alpha=0.7, color='red', edgecolor='black')\n",
    "    axes[0, 1].set_title(f'Distribution of {numeric_cols[0]} (with outliers)')\n",
    "    axes[0, 1].set_xlabel('Value')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Categorical distribution\n",
    "cat_cols = raw_data.select_dtypes(include=['object']).columns\n",
    "if len(cat_cols) > 0:\n",
    "    raw_data[cat_cols[0]].value_counts().plot(kind='bar', ax=axes[1, 0], color='orange', alpha=0.7)\n",
    "    axes[1, 0].set_title(f'Distribution of {cat_cols[0]}')\n",
    "    axes[1, 0].set_xlabel('Categories')\n",
    "    axes[1, 0].set_ylabel('Count')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Correlation heatmap (numeric features only)\n",
    "numeric_data = raw_data.select_dtypes(include=[np.number])\n",
    "if numeric_data.shape[1] > 1:\n",
    "    correlation_matrix = numeric_data.corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('Feature Correlations')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('../outputs/figures/raw_data_quality_assessment.png', dpi=300, bbox_inches='tight')\n",
    "print(\"📈 Data quality visualization saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57379358",
   "metadata": {},
   "source": [
    "## 6. Outlier Detection and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e22d559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers in numeric columns\n",
    "def detect_outliers_iqr(series, factor=1.5):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - factor * IQR\n",
    "    upper_bound = Q3 + factor * IQR\n",
    "    return (series < lower_bound) | (series > upper_bound)\n",
    "\n",
    "print(\"=== OUTLIER ANALYSIS ===\")\n",
    "for col in numeric_cols:\n",
    "    outliers = detect_outliers_iqr(raw_data[col].dropna())\n",
    "    outlier_count = outliers.sum()\n",
    "    outlier_percentage = (outlier_count / len(raw_data)) * 100\n",
    "    print(f\"{col}: {outlier_count} outliers ({outlier_percentage:.1f}%)\")\n",
    "    \n",
    "    if outlier_count > 0:\n",
    "        # Visualize outliers\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.boxplot(raw_data[col].dropna())\n",
    "        plt.title(f'Box Plot: {col}')\n",
    "        plt.ylabel('Value')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.scatter(range(len(raw_data)), raw_data[col], alpha=0.6, c='blue', label='Normal')\n",
    "        outlier_mask = detect_outliers_iqr(raw_data[col].fillna(raw_data[col].median()))\n",
    "        plt.scatter(np.where(outlier_mask)[0], raw_data[col][outlier_mask], \n",
    "                   c='red', alpha=0.8, label='Outliers')\n",
    "        plt.title(f'Outlier Detection: {col}')\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel('Value')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        break  # Show only first column with outliers for brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d132e9b",
   "metadata": {},
   "source": [
    "## 7. Data Story Setup - Key Insights\n",
    "\n",
    "Based on our exploration, let's document the key insights that will drive our data story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25537eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize key findings for our data story\n",
    "story_insights = {\n",
    "    'dataset_shape': raw_data.shape,\n",
    "    'missing_data_issues': sum(1 for x in quality_report['missing_values'].values() if x > 0),\n",
    "    'duplicate_rows': quality_report['duplicates'],\n",
    "    'outlier_columns': [],\n",
    "    'data_types_mixed': len(quality_report['numeric_columns']) + len(quality_report['categorical_columns'])\n",
    "}\n",
    "\n",
    "# Count outlier columns\n",
    "for col in numeric_cols:\n",
    "    outliers = detect_outliers_iqr(raw_data[col].dropna())\n",
    "    if outliers.sum() > 0:\n",
    "        story_insights['outlier_columns'].append(col)\n",
    "\n",
    "print(\"=== DATA STORY INSIGHTS ===\")\n",
    "print(f\"📊 Dataset contains {story_insights['dataset_shape'][0]:,} rows and {story_insights['dataset_shape'][1]} columns\")\n",
    "print(f\"❌ {story_insights['missing_data_issues']} columns have missing values\")\n",
    "print(f\"🔄 {story_insights['duplicate_rows']} duplicate rows found\")\n",
    "print(f\"⚠️  {len(story_insights['outlier_columns'])} columns contain outliers\")\n",
    "print(f\"🔢 Mixed data types: {story_insights['data_types_mixed']} total columns\")\n",
    "\n",
    "print(\"\\n=== STORY NARRATIVE POINTS ===\")\n",
    "print(\"1. Raw data contains significant quality issues\")\n",
    "print(\"2. Missing values may bias analysis results\")\n",
    "print(\"3. Outliers could skew model performance\")\n",
    "print(\"4. Duplicates inflate dataset size artificially\")\n",
    "print(\"5. Data preparation is essential for reliable insights\")\n",
    "\n",
    "# Save insights for later use\n",
    "import json\n",
    "with open('../outputs/reports/raw_data_insights.json', 'w') as f:\n",
    "    json.dump(story_insights, f, indent=2, default=str)\n",
    "\n",
    "print(\"\\n💾 Insights saved for storytelling!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce012dff",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "Based on our exploration, the next notebook should focus on:\n",
    "\n",
    "1. **Data Cleaning**: Handle missing values, remove duplicates\n",
    "2. **Outlier Treatment**: Apply appropriate outlier handling strategies\n",
    "3. **Feature Engineering**: Create new features, encode categorical variables\n",
    "4. **Data Validation**: Ensure data quality improvements\n",
    "\n",
    "This will set up the perfect comparison for our data storytelling narrative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0b9c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save raw data for use in next notebook\n",
    "raw_data.to_csv('../data/raw/sample_raw_data.csv', index=False)\n",
    "print(\"✅ Raw data saved for processing pipeline!\")\n",
    "print(\"\\n🚀 Ready to move to notebook 02_data_preparation.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
